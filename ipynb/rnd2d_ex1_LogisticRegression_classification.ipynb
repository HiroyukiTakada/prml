{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnd2d_ex1_LogisticRegression_classification.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"}},"cells":[{"metadata":{"id":"2Xf5xoadG9Z0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UW1hl94VG9Z8","colab_type":"text"},"cell_type":"markdown","source":["Logistic regression of a 2D data set\n","==================="]},{"metadata":{"id":"PCg8jJ2AG9aH","colab_type":"text"},"cell_type":"markdown","source":["Define functions\n","----------------"]},{"metadata":{"id":"NJxRr0_4G9aI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","\n","# Linear descriminant function\n","def g(X, w):\n","    return w[0] + np.dot(X, w[1:])\n","\n","# Logistic function\n","def sigmoid(z):\n","    return 1.0 / (1 + np.exp(-z))\n","\n","# Probability P(y=+1 | X)\n","def predict(X, w):\n","    return sigmoid(g(X, w))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EaibuLlOG9aL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Logistic loss function\n","def loss_function(X, y, w):    # X(n,d), y(n,), w(d+1,)\n","    n = len(y)\n","    predictions = predict(X, w)\n","    return np.where( y > 0, - np.log(predictions), -np.log(1-predictions) ).sum() / n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H_oYcRCTG9aO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Gradient descent (GD) algorithm\n","def update_weights(X, y, w, lr):    # X(n,d), y(n,), w(d+1,)\n","\n","    n, d = X.shape\n","    predictions = predict(X, w)\n","\n","    # Compute the gradient of the logistic loss function\n","    gradient = np.zeros(d+1)\n","    gradient[0] = np.ones(n).T.dot(np.where( y > 0, predictions-1, predictions )) / n\n","    gradient[1:] = X.T.dot(np.where( y > 0, predictions-1, predictions )) / n\n","    \n","    # Update by subtraction\n","    w -= lr * gradient\n","\n","    return w"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kA1AL6EWG9aR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Training by GD for a given training dataset (X_train, y_train)\n","def train(X_train, y_train, lr=1.0, w=None, iters=100):\n","\n","    d = X_train.shape[1]\n","    if w is None:\n","        w = np.zeros(d+1)\n","\n","    cost_history = []\n","    every = iters // 10\n","    for i in range(iters):\n","        w = update_weights(X_train, y_train, w, lr)\n","\n","        # Calculate error for auditing purposes\n","        cost = loss_function(X_train, y_train, w)\n","        cost_history.append(cost)\n","\n","        # Log Progress\n","        if i % every == 0:\n","            print(\"iter: \"+str(i) + \" cost: \"+str(cost) + str(w))\n","    return w, cost_history"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-PbAn1DPG9aX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from matplotlib import pyplot as plt\n","# Visualization of the decision boundary and regions\n","def plot2d_classification(decision_function, X_train, y_train, X_test=None, y_test=None, w=None, cmap=plt.cm.bwr, xlim=None, ylim=None, levels=None, colors='k', linestyles=None):\n","\n","    plt.figure()\n","    ax = plt.axes()\n","\n","    if xlim is None:\n","        xlim = [X_train[:, 0].min() - .5, X_train[:, 0].max() + .5]\n","    if ylim is None:\n","        ylim = [X_train[:, 1].min() - .5, X_train[:, 1].max() + .5]\n","\n","    xx, yy = np.meshgrid(np.arange(xlim[0], xlim[1], 0.02), np.arange(ylim[0], ylim[1], 0.02))    \n","\n","    # Show prediction (P(y=+1 | X) by color by assigning a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n","    Z = decision_function(np.c_[xx.ravel(), yy.ravel()])\n","    # Put the result into a color plot\n","    Z = Z.reshape(xx.shape)\n","    if levels is not None:\n","        ax.contour(xx, yy, Z, levels=levels, colors=colors, linestyles=linestyles, alpha=0.5)\n","    else:\n","        ax.pcolor(xx, yy, Z, cmap=cmap, alpha=0.1, edgecolors=None)\n","\n","    # Plot the decision boundary\n","    if w is not None:\n","        x1 = np.linspace(xx.min(), xx.max(), 100)\n","        if w[2] != 0:\n","            x2 = -(w[0] + w[1] * x1) / w[2]\n","            cnd = np.logical_and(x2<yy.max(), x2>yy.min())\n","            plt.plot(x1[cnd], x2[cnd], 'k-')\n","        else:\n","            plt.axvline(x=-w[0]/w[1], color='k')\n","\n","    # Plot also the training points\n","    ax.scatter(X_train[y_train>0, 0], X_train[y_train>0, 1], c='r',  marker='s', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n","    ax.scatter(X_train[y_train<=0, 0], X_train[y_train<=0, 1], c='b', marker='o', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n","    # and testing points if given\n","    if X_test is not None and y_test is not None:\n","        ax.scatter(X_test[y_test>0, 0], X_test[y_test>0, 1], c='r',  marker='s', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n","        ax.scatter(X_test[y_test<=0, 0], X_test[y_test<=0, 1], c='b', marker='o', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n","        plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n","        ax.get_legend().legendHandles[0].set_color('k')\n","        ax.get_legend().legendHandles[1].set_color('k')\n","\n","    ax.set_xlim(xx.min(), xx.max())\n","    ax.set_ylim(yy.min(), yy.max())\n","    plt.axis('tight')\n","    plt.xlabel('x1', fontsize=16)\n","    plt.ylabel('x2', fontsize=16)\n","    plt.xticks(fontsize=16)\n","    plt.yticks(fontsize=16)\n","    plt.gca().set_aspect('equal')\n","    plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7bCIjUckNOwF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def  histogram_predict(decision_function, X_train, y_train, X_test=None, y_test=None, bins=None, normed=False):\n","    if bins is None:\n","        bins = len(y_train) // 4\n","    plt.figure()\n","    ax = plt.axes()\n","    pred = decision_function(X_train)\n","    plt.hist( [ pred[y_train>0], pred[y_train<=0] ], bins=bins, histtype='stepfilled', normed=normed, alpha=0.5, color=['r', 'b'], label=['$y=+1$', '$y=-1$'])\n","    if X_test is not None and y_test is not None:\n","        pred = decision_function(X_test)\n","        plt.hist( [ pred[y_test>0], pred[y_test<=0] ], bins=bins, histtype='stepfilled', normed=normed, alpha=0.3, color=['r', 'b'], label=['$y_{test}=+1$', '$y_{test}=-1$'])\n","    plt.xlabel(\"$g(x)$\", fontsize=16)\n","    plt.ylabel(\"Frequency\", fontsize=16)\n","    plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n","    plt.axis('tight')\n","    plt.xticks(fontsize=16)\n","    plt.yticks(fontsize=16)\n","    from matplotlib.ticker import FormatStrFormatter\n","    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n","    plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"obFLzXN6G9Z_","colab_type":"text"},"cell_type":"markdown","source":["Make training data\n","------------------"]},{"metadata":{"id":"aQWiGCU0G9aA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Example 1: define manually\n","X = np.array([[0, 0], [1,0], [0,1], [1,1]])\n","y = np.array([-1,1,1,1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qPcFoZwoU2fp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Example 2: draw npos and nneg points from the Gaussian distribution for each class\n","npos = 30\n","nneg = 30\n","np.random.seed(321)\n","X = np.r_[np.random.randn(npos, 2) + [3, 3], np.random.randn(nneg, 2)]\n","# [1,1,...,1,-1,-1,...,-1]\n","y = np.array([1] * npos + [-1] * nneg)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IAcLs8fxU3b8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Example 3: create moons using sklearn\n","from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=0)\n","y[y==0] = -1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tbqXssHcU5Is","colab_type":"text"},"cell_type":"markdown","source":["Plot the training points"]},{"metadata":{"id":"cl-028CoG9aE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Plot the training points\n","ax = plt.figure()\n","ax = plt.axes()\n","ax.scatter(X[y>0, 0], X[y>0, 1], c='r',  marker='s', cmap=plt.cm.bwr, edgecolors='k', label='Training data', alpha=1)\n","ax.scatter(X[y<=0, 0], X[y<=0, 1], c='b', marker='o', cmap=plt.cm.bwr, edgecolors='k', label='Training data', alpha=1)\n","plt.xlabel('x1', fontsize=16)\n","plt.ylabel('x2', fontsize=16)\n","plt.xticks(fontsize=16)\n","plt.yticks(fontsize=16)\n","plt.gca().set_aspect('equal')\n","ax.set_xlim(X[:,0].min()-0.5, X[:,0].max()+0.5)\n","ax.set_ylim(X[:,1].min()-0.5, X[:,1].max()+0.5)\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pT0P8z26G9aT","colab_type":"text"},"cell_type":"markdown","source":["Run the training\n","----------------"]},{"metadata":{"id":"NOrZbPLsG9aU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["w, ch = train(X, y, lr=1., iters=200, w=np.ones(3))\n","print(w)\n","plt.plot(ch)\n","plt.xlabel(\"Iteration\", fontsize=16)\n","plt.ylabel(\"Loss\", fontsize=16)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LmEOwqiqG9ab","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# visualize sigmoid(g(X,w)) on training data (X,y)\n","plot2d_classification(lambda X: predict(X,w), X, y,w=w)\n","plt.savefig('rnd2d_ex1_logistic_regression.png', transparent=True)\n","histogram_predict(lambda X: g(X,w), X, y)\n","#histogram_predict(lambda X: predict(X,w), X, y)\n","plt.savefig('hist_rnd2d_ex1_logistic_regression.png', transparent=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hgYDQcqGG9ad","colab_type":"text"},"cell_type":"markdown","source":["Logistic regression using scikit-learn\n","--------------------------------------"]},{"metadata":{"id":"OA7s-t0MG9ae","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import sklearn\n","from sklearn.linear_model import LogisticRegression\n","\n","model = LogisticRegression()\n","model.fit(X,y)\n","\n","w_skl = np.c_[model.intercept_, model.coef_].ravel()\n","print(w_skl)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eIQAY-CgG9ah","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# visualize sigmoid(g(X,w)) by color\n","plot2d_classification(lambda X: model.predict_proba(X)[:,1], X, y, w=w_skl)\n","histogram_predict(lambda X: model.decision_function(X), X, y)\n","#histogram_predict(lambda X: predict(X,w_skl), X, y)"],"execution_count":0,"outputs":[]}]}